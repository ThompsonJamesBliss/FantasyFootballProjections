{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Fantasy Football Projections based on 2009-18 statistics.\n",
    "\n",
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns_stats = [\"Season\", \"playerID\", \"game.id\", \"Team\", \"name\", \"pass.att\", \"pass.comp\",\n",
    "                    \"passyds\", \"pass.tds\", \"pass.ints\", \"rush.att\", \"rushyds\",\n",
    "                    \"rushtds\", \"recyds\", \"rec.tds\", \"recept\", \"kickret.avg\",\n",
    "                   \"kickret.tds\", \"kick.rets\", \"punt.rets\", \"puntret.avg\", \n",
    "                   \"puntret.tds\", \"fgm\", \"xpmade\",\n",
    "                   \"sacks\", 'defints', \"forced.fumbs\", \"totalfumbs\", \"recfumbs\", \"pass.twoptm\",\n",
    "                   \"fumbslost\", \"rec.twoptm\", \"rush.twoptm\"]\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(\"2009stats.csv\", usecols = relevant_columns_stats)\n",
    "df_train = df_train.append(pd.read_csv(\"2010stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2011stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2012stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2013stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2014stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2015stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2016stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2017stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train = df_train.append(pd.read_csv(\"2018stats.csv\", usecols = relevant_columns_stats))\n",
    "df_train.Team = df_train.Team.replace(\"STL\",\"LA\")\n",
    "df_train.Team = df_train.Team.replace(\"SD\",\"LAC\")\n",
    "df_train.Team = df_train.Team.replace(\"JAC\",\"JAX\")\n",
    "\n",
    "\n",
    "df_train.dropna(inplace = True)\n",
    "\n",
    "relevant_columns_def = [\"Tm\",\"PR TD\",\"KR TD\",\"FblTD\",\"IntTD\",\"OthTD\",\"Sfty\"]\n",
    "\n",
    "relevant_columns_def = [\"Tm\",\"PR TD\",\"KR TD\",\"FblTD\",\"IntTD\",\"OthTD\",\"Sfty\"]\n",
    "\n",
    "df_def = pd.read_csv(\"2009def.csv\", usecols = relevant_columns_def)\n",
    "df_def = df_def.append(pd.read_csv(\"2010def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2011def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2012def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2013def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2014def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2015def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2016def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2017def.csv\", usecols = relevant_columns_def))\n",
    "df_def = df_def.append(pd.read_csv(\"2018def.csv\", usecols = relevant_columns_def))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Attributes for Opposition and Making Way to Merge Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_def[\"sum_vals\"] = 1\n",
    "\n",
    "df_abbreviations = pd.read_csv(\"abbreviations.csv\")\n",
    "\n",
    "df_def = df_def.merge(df_abbreviations, on = [\"Tm\"])\n",
    "\n",
    "\n",
    "df_def[\"Season\"] = df_def.groupby(\"Team\")[\"sum_vals\"].transform(lambda x: x.cumsum() + 2008)\n",
    "\n",
    "df_def.fillna(0, inplace = True)\n",
    "\n",
    "df_def.drop([\"Tm\", \"sum_vals\"], inplace = True, axis = 1)\n",
    "\n",
    "df_train['opp'] = df_train.groupby(\"game.id\").Team.transform(lambda x: x.unique()[0])\n",
    "df_train['opp2'] = df_train.groupby(\"game.id\").Team.transform(lambda x: x.unique()[1])\n",
    "\n",
    "df_train.loc[df_train['opp'] == df_train['Team'], 'opp'] = df_train.loc[df_train['opp'] == df_train['Team'], 'opp2']\n",
    "df_train.drop(['opp2'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns_pos = [\"GSIS_ID\", \"Season\", \"Pos\"]\n",
    "\n",
    "df_pos = pd.read_csv(\"2009Pos.csv\", usecols = relevant_columns_pos)\n",
    "df_pos = df_pos.append(pd.read_csv(\"2010Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2011Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2012Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2013Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2014Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2015Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2016Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2017Pos.csv\", usecols = relevant_columns_pos))\n",
    "df_pos = df_pos.append(pd.read_csv(\"2018Pos.csv\", usecols = relevant_columns_pos))\n",
    "\n",
    "df_pos.Pos = df_pos.Pos.replace(\"FB\",\"RB\")\n",
    "\n",
    "df_pos = df_pos.loc[df_pos.Pos != \"K\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Data into Offense and Defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_columns = [\"Team\",\"rec.twoptm\",\n",
    "              \"passyds\", \"pass.tds\", \"rushyds\",\n",
    "              \"rushtds\", \"kickret.tds\", \"puntret.tds\", \"fgm\",\n",
    "              \"xpmade\", \"fumbslost\", \"pass.ints\",\n",
    "              \"rush.twoptm\"]\n",
    "\n",
    "df_defense = df_train\n",
    "\n",
    "df_defense[\"points_allowed\"] = ((df_defense[\"pass.tds\"] + df_defense[\"rushtds\"] + df_defense[\"kickret.tds\"]\n",
    "                                 + df_defense[\"puntret.tds\"]) * 6 + df_defense[\"fgm\"] * 3 +\n",
    "                                df_defense[\"rec.twoptm\"] * 2 + df_defense[\"rush.twoptm\"] * 2\n",
    "                                + df_defense[\"xpmade\"])\n",
    "\n",
    "df_defense[\"yards\"] = df_defense[\"passyds\"] + df_defense[\"rushyds\"]\n",
    "\n",
    "df_defense = df_defense.groupby([\"game.id\",\"opp\", \"Season\",])[\"yards\", \"points_allowed\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_defense[\"0pts\"] = df_defense.points_allowed == 0\n",
    "df_defense[\"1_6pts\"] = df_defense.points_allowed.between(1, 6)\n",
    "df_defense[\"7_13pts\"] = df_defense.points_allowed.between(7, 13)\n",
    "df_defense[\"14_17pts\"] = df_defense.points_allowed.between(14, 17)\n",
    "df_defense[\"18_27pts\"] = df_defense.points_allowed.between(18, 27)\n",
    "df_defense[\"28_34pts\"] = df_defense.points_allowed.between(28, 34)\n",
    "df_defense[\"35_45pts\"] = df_defense.points_allowed.between(35, 45)\n",
    "df_defense[\"45pts\"] = df_defense.points_allowed > 45\n",
    "\n",
    "df_defense[\"YA100\"] = df_defense.yards < 100\n",
    "df_defense[\"YA199\"] = df_defense.yards.between(100, 199)\n",
    "df_defense[\"YA299\"] = df_defense.yards.between(200, 299)\n",
    "df_defense[\"YA349\"] = df_defense.yards.between(300, 349)\n",
    "df_defense[\"YA399\"] = df_defense.yards.between(350, 399)\n",
    "df_defense[\"YA449\"] = df_defense.yards.between(400, 449)\n",
    "df_defense[\"YA499\"] = df_defense.yards.between(450, 499)\n",
    "df_defense[\"YA549\"] = df_defense.yards.between(500, 549)\n",
    "df_defense[\"YA550\"] = df_defense.yards > 549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_columns = [\"0pts\", \"1_6pts\", \"7_13pts\", \"14_17pts\",\n",
    "               \"28_34pts\",\"35_45pts\",\"45pts\",\n",
    "               \"YA100\",\"YA199\",\"YA299\",\"YA399\",\n",
    "               \"YA449\",\"YA499\",\"YA549\",\"YA550\", \"YA349\", \"18_27pts\"]\n",
    "\n",
    "\n",
    "df_defense = df_defense.groupby([\"Season\", \"opp\"])[def_columns].sum()\n",
    "\n",
    "\n",
    "\n",
    "df_defense[\"Sacks\"] = df_train.groupby([\"Season\", \"Team\"])[\"sacks\"].sum()\n",
    "\n",
    "df_train[\"exp\"] = df_train.groupby(\"playerID\").Season.transform(lambda x: x - min(x))\n",
    "\n",
    "df_defense[\"exp\"] = df_train.groupby([\"Season\", \"Team\"])[\"exp\"].mean()\n",
    "\n",
    "df_train.drop(\"exp\", inplace = True, axis = 1)\n",
    "\n",
    "df_defense[\"pass.ints\"] = df_train.groupby([\"Season\", \"opp\"])[\"pass.ints\"].sum()\n",
    "\n",
    "df_defense[\"fumbslost\"] = df_train.groupby([\"Season\", \"opp\"])[\"fumbslost\"].sum()\n",
    "\n",
    "df_defense[\"rush_per_att\"] = (df_train.groupby([\"Season\", \"opp\"])[\"rushyds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"rush.att\"].sum())\n",
    "\n",
    "df_defense[\"td_rush_per_att\"] = (df_train.groupby([\"Season\", \"opp\"])[\"rushtds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"rush.att\"].sum())\n",
    "\n",
    "df_defense[\"rec_per_att\"] = (df_train.groupby([\"Season\", \"opp\"])[\"recyds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"recept\"].sum())\n",
    "\n",
    "df_defense[\"td_rec_per_att\"] = (df_train.groupby([\"Season\", \"opp\"])[\"rec.tds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"recept\"].sum())\n",
    "\n",
    "df_defense[\"pass_per_att\"] = (df_train.groupby([\"Season\", \"opp\"])[\"passyds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"pass.att\"].sum())\n",
    "\n",
    "df_defense[\"td_pass_per_att\"] = (df_train.groupby([\"Season\", \"opp\"])[\"pass.tds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"pass.att\"].sum())\n",
    "\n",
    "df_defense[\"pass_per_comp\"] = (df_train.groupby([\"Season\", \"opp\"])[\"passyds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"pass.comp\"].sum())\n",
    "\n",
    "df_defense[\"td_pass_per_comp\"] = (df_train.groupby([\"Season\", \"opp\"])[\"pass.tds\"].sum() / \n",
    "                                df_train.groupby([\"Season\", \"opp\"])[\"pass.comp\"].sum())\n",
    "\n",
    "\n",
    "\n",
    "df_defense = df_defense.merge(df_def, left_on = [\"opp\", \"Season\"], right_on = [\"Team\", \"Season\"])\n",
    "\n",
    "df_defense[\"TD\"] = (df_defense[\"PR TD\"] + df_defense[\"KR TD\"] + df_defense[\"FblTD\"] + df_defense[\"IntTD\"] +\n",
    "+df_defense[\"OthTD\"])\n",
    "\n",
    "df_defense[\"turnovers\"] = df_defense[\"fumbslost\"] + df_defense[\"pass.ints\"]\n",
    "\n",
    "df_defense.drop([\"fumbslost\", \"pass.ints\", \n",
    "            \"PR TD\",\"KR TD\",\"FblTD\",\"IntTD\",\"OthTD\"], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "df_defense_scores = (df_defense[\"TD\"]*6 + df_defense[\"turnovers\"] * 2 + df_defense[\"Sacks\"] + df_defense[\"Sfty\"] * 2 +\n",
    "                    (df_defense[\"0pts\"] + df_defense[\"YA100\"]) * 5 + \n",
    "                    df_defense['1_6pts'] * 4 +\n",
    "                    (df_defense[\"7_13pts\"] + df_defense[\"YA199\"]) * 3 +\n",
    "                     df_defense[\"YA299\"] * 2 +\n",
    "                    df_defense[\"14_17pts\"] +\n",
    "                    (df_defense[\"YA399\"] + df_defense[\"28_34pts\"]) * (-1) +\n",
    "                    (df_defense[\"YA449\"] + df_defense[\"35_45pts\"]) * (-3) +\n",
    "                    (df_defense[\"YA499\"] + df_defense[\"45pts\"]) * (-5) +\n",
    "                    (df_defense[\"YA549\"]) * (-6) +\n",
    "                    (df_defense[\"YA550\"]) * (-7)).to_frame()\n",
    "\n",
    "off_columns = [\"Season\", \"playerID\", \"game.id\", \"Team\", \"name\", \"pass.att\", \"pass.comp\",\n",
    "                    \"passyds\", \"pass.tds\", \"pass.ints\", \"rush.att\", \"rushyds\",\n",
    "                    \"rushtds\", \"recyds\", \"rec.tds\", \"recept\", \"kickret.avg\",\n",
    "                   \"kickret.tds\", \"kick.rets\", \"punt.rets\", \"puntret.avg\", \n",
    "                   \"puntret.tds\", \"fumbslost\", \"rec.twoptm\", \"rush.twoptm\", \"pass.twoptm\",\n",
    "              \"totalfumbs\", \"opp\"]\n",
    "\n",
    "off_columns_numeric = [\"pass.att\", \"pass.comp\", \"totalfumbs\", \"fumbslost\",\n",
    "                    \"passyds\", \"pass.tds\", \"pass.ints\", \"rush.att\", \"rushyds\",\n",
    "                    \"rushtds\", \"recyds\", \"rec.tds\", \"recept\", \"kickret.avg\",\n",
    "                   \"kickret.tds\", \"kick.rets\", \"punt.rets\", \"puntret.avg\", \n",
    "                   \"puntret.tds\", \"rec.twoptm\", \"rush.twoptm\", \"pass.twoptm\"]\n",
    "\n",
    "\n",
    "\n",
    "df_offense = df_train[off_columns]\n",
    "\n",
    "\n",
    "df_offense = pd.merge(df_offense, df_pos, left_on = [\"playerID\", \"Season\"], \n",
    "                      right_on = [\"GSIS_ID\", \"Season\"], how = \"left\").drop(\"GSIS_ID\", axis = 1)\n",
    "\n",
    "df_offense.Pos = df_offense.groupby(\"playerID\").Pos.fillna(method = 'ffill')\n",
    "df_offense.Pos = df_offense.groupby(\"playerID\").Pos.fillna(method = 'bfill')\n",
    "\n",
    "df_offense = df_offense.dropna(subset = [\"Pos\"])\n",
    "\n",
    "\n",
    "\n",
    "df_offense['n_games'] = df_offense.groupby(['playerID', 'Season'])['game.id'].transform(lambda x: len(x.unique()))\n",
    "\n",
    "\n",
    "df_offense[\"points\"] = (df_offense[\"passyds\"] * 0.025 + 4 * df_offense[\"pass.tds\"] - 2 * df_offense[\"pass.ints\"]\n",
    "                       + 0.1 * df_offense[\"rushyds\"] + 6 * df_offense[\"rushtds\"] + 0.1 * df_offense[\"recyds\"] +\n",
    "                        6 * df_offense[\"rec.tds\"] + 6 * df_offense[\"kickret.tds\"] + 6 * df_offense[\"puntret.tds\"]\n",
    "                       + 2 * df_offense[\"rec.twoptm\"] + 2 * df_offense[\"rush.twoptm\"] + 2 * df_offense[\"pass.twoptm\"]\n",
    "                        - 2 * df_offense[\"fumbslost\"])\n",
    "\n",
    "off_columns_numeric.append('points')\n",
    "\n",
    "off_columns_numeric_no_avg = off_columns_numeric.copy()\n",
    "\n",
    "\n",
    "for col in off_columns_numeric_no_avg:    \n",
    "    df_offense[col + '_per_game'] = df_offense[col] / df_offense['n_games']\n",
    "    \n",
    "    df_offense[col + '_var'] = ((df_offense[col] - \n",
    "                                 df_offense.groupby([\"Season\", \"playerID\"])[col + '_per_game'].transform(lambda x: sum(x))\n",
    "                                ) ** 2/df_offense['n_games'])\n",
    "    \n",
    "    df_offense[col + '_var'] = ((df_offense[col] - \n",
    "                                 df_offense.groupby([\"playerID\"])[col + '_per_game'].transform(lambda x: sum(x))\n",
    "                                ) ** 2/df_offense['n_games'])\n",
    "    \n",
    "    off_columns_numeric.append(col + '_per_game')\n",
    "    \n",
    "    off_columns_numeric.append(col + '_var')\n",
    "\n",
    "off_columns_numeric.append('n_games')\n",
    "\n",
    "\n",
    "\n",
    "df_offense.Team = df_offense.groupby([\"Season\", \"playerID\"]).Team.transform(lambda x: x.iloc[len(x) - 1])\n",
    "\n",
    "df_offense = pd.merge(df_offense.groupby([\"Season\", \"playerID\", \"Team\"])[off_columns_numeric].sum().reset_index(),\n",
    "                      df_offense[[\"playerID\", \"name\"]].drop_duplicates(\"playerID\", keep = \"last\"), on = \"playerID\")\n",
    "\n",
    "\n",
    "df_offense['n_games'] = np.sqrt(df_offense['n_games'])\n",
    "\n",
    "df_offense[\"points\"] = (df_offense[\"passyds\"] * 0.025 + 4 * df_offense[\"pass.tds\"] - 2 * df_offense[\"pass.ints\"]\n",
    "                       + 0.1 * df_offense[\"rushyds\"] + 6 * df_offense[\"rushtds\"] + 0.1 * df_offense[\"recyds\"] +\n",
    "                        6 * df_offense[\"rec.tds\"] + 6 * df_offense[\"kickret.tds\"] + 6 * df_offense[\"puntret.tds\"]\n",
    "                       + 2 * df_offense[\"rec.twoptm\"] + 2 * df_offense[\"rush.twoptm\"] + 2 * df_offense[\"pass.twoptm\"]\n",
    "                       - 2 * df_offense[\"fumbslost\"])\n",
    "\n",
    "\n",
    "\n",
    "df_offense['total_games'] = df_offense.groupby([\"playerID\"])['n_games'].transform(lambda x: np.cumsum(x))\n",
    "\n",
    "off_columns_numeric.append('total_games')\n",
    "\n",
    "for col in off_columns_numeric_no_avg:\n",
    "    df_offense[col + \"_career\"] = df_offense.groupby([\"playerID\"])[col].transform(lambda x: np.cumsum(x))\n",
    "    \n",
    "    df_offense[col + \"_career_per_game\"] = df_offense.groupby([\"playerID\"])[col].transform(lambda x: \n",
    "                                                                                           np.cumsum(x))/df_offense['total_games']\n",
    "\n",
    "    off_columns_numeric.append(col + \"_career\")\n",
    "    \n",
    "    off_columns_numeric.append(col + \"_career_per_game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offense['rush_per_att'] = (df_offense['rushyds'] / df_offense['rush.att']).fillna(0)\n",
    "df_offense['rush_per_att'].loc[df_offense['rush_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('rush_per_att')\n",
    "\n",
    "df_offense['pass_per_att'] = (df_offense['passyds'] / df_offense['pass.att']).fillna(0)\n",
    "df_offense['pass_per_att'].loc[df_offense['pass_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('pass_per_att')\n",
    "\n",
    "\n",
    "df_offense['pass_per_comp'] = (df_offense['passyds'] / df_offense['pass.comp']).fillna(0)\n",
    "df_offense['pass_per_comp'].loc[df_offense['pass_per_comp'] == np.inf] = 0\n",
    "off_columns_numeric.append('pass_per_comp')\n",
    "\n",
    "\n",
    "df_offense['rec_per_att'] = (df_offense['recyds'] / df_offense['recept']).fillna(0)\n",
    "df_offense['rec_per_att'].loc[df_offense['rec_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('rec_per_att')\n",
    "\n",
    "df_offense['td_rush_per_att'] = (df_offense['rushtds'] / df_offense['rush.att']).fillna(0)\n",
    "df_offense['td_rush_per_att'].loc[df_offense['td_rush_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('td_rush_per_att')\n",
    "\n",
    "df_offense['td_pass_per_att'] = (df_offense['pass.tds'] / df_offense['pass.att']).fillna(0)\n",
    "df_offense['td_pass_per_att'].loc[df_offense['td_pass_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('td_pass_per_att')\n",
    "\n",
    "\n",
    "df_offense['td_pass_per_comp'] = (df_offense['pass.tds'] / df_offense['pass.comp']).fillna(0)\n",
    "df_offense['td_pass_per_comp'].loc[df_offense['td_pass_per_comp'] == np.inf] = 0\n",
    "off_columns_numeric.append('td_pass_per_comp')\n",
    "\n",
    "\n",
    "df_offense['td_rec_per_att'] = (df_offense['rec.tds'] / df_offense['recept']).fillna(0)\n",
    "df_offense['td_rec_per_att'].loc[df_offense['td_rec_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('td_rec_per_att')\n",
    "\n",
    "\n",
    "df_offense['fumb_rush_per_att'] = (df_offense['totalfumbs'] / df_offense['rush.att']).fillna(0)\n",
    "df_offense['fumb_rush_per_att'].loc[df_offense['fumb_rush_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('fumb_rush_per_att')\n",
    "\n",
    "df_offense['fumb_pass_per_att'] = (df_offense['totalfumbs'] / df_offense['pass.att']).fillna(0)\n",
    "df_offense['fumb_pass_per_att'].loc[df_offense['fumb_pass_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('fumb_pass_per_att')\n",
    "\n",
    "df_offense['int_pass_per_att'] = (df_offense['pass.ints'] / df_offense['pass.att']).fillna(0)\n",
    "df_offense['int_pass_per_att'].loc[df_offense['int_pass_per_att'] == np.inf] = 0\n",
    "off_columns_numeric.append('int_pass_per_att')\n",
    "\n",
    "df_offense['fumb_per_rec'] = (df_offense['totalfumbs'] / df_offense['recept']).fillna(0)\n",
    "df_offense['fumb_per_rec'].loc[df_offense['fumb_per_rec'] == np.inf] = 0\n",
    "off_columns_numeric.append('fumb_per_rec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data for previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offense = pd.merge(df_offense, df_pos, left_on = [\"playerID\", \"Season\"], \n",
    "                      right_on = [\"GSIS_ID\", \"Season\"], how = \"left\").drop(\"GSIS_ID\", axis = 1)\n",
    "\n",
    "df_offense.Pos = df_offense.groupby(\"playerID\").Pos.fillna(method = 'ffill')\n",
    "df_offense.Pos = df_offense.groupby(\"playerID\").Pos.fillna(method = 'bfill')\n",
    "\n",
    "relevant_prev_columns = off_columns_numeric.copy()\n",
    "\n",
    "relevant_columns_spotrac = ['PLAYER', 'POS', 'TEAM']\n",
    "\n",
    "df_spotrac = pd.read_csv(\"Spotrac_NFLActivePlayerContracts.csv\", encoding = \"ISO-8859-1\",\n",
    "                        usecols = relevant_columns_spotrac)\n",
    "\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'OLB']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'DT']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'DE']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'LT']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'ILB']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'G']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'FS']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'CB']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'RT']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'SS']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'C']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'P']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'LB']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'T']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'S']\n",
    "df_spotrac = df_spotrac.loc[df_spotrac.POS != 'LS']\n",
    "\n",
    "df_spotrac.PLAYER = df_spotrac.PLAYER.transform(lambda x: x.str.split('\\xa0').str.get(0))\n",
    "df_spotrac.PLAYER = df_spotrac.PLAYER.transform(lambda x: x.str.split(' ').str.get(0).str[0] + '.' + \n",
    "                                         x.str.split(' ').str.get(-1))\n",
    "\n",
    "df_spotrac.TEAM = df_spotrac.TEAM.transform(lambda x: x.str.split('Signed').str.get(0))\n",
    "df_spotrac.rename(columns={\"PLAYER\": \"name\", \"TEAM\": \"next_Team\", \"POS\": \"Pos\"}, inplace = True)\n",
    "\n",
    "df_spotrac.loc[df_spotrac.name == \"O.Jr.\", \"name\"] = \"O.Beckham\"\n",
    "df_spotrac.loc[df_spotrac.name == \"T.Jr.\", \"name\"] = \"T.Ginn\"\n",
    "\n",
    "df_spotrac[\"name_dict\"] = df_spotrac[\"name\"] + \"_\" + df_spotrac[\"Pos\"]\n",
    "\n",
    "df_offense.sort_values([\"Season\", \"Team\"], inplace = True)\n",
    "df_offense.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df_offense[\"name_dict\"] = df_offense[\"name\"] + \"_\" + df_offense[\"Pos\"]\n",
    "\n",
    "df_offense[\"next_Team\"] = df_offense.groupby(\"playerID\").Team.transform(lambda x: x.shift(-1))\n",
    "df_offense[\"prev_Team\"] = df_offense.groupby(\"playerID\").Team.transform(lambda x: x.shift())\n",
    "df_offense.next_Team = df_offense.set_index(\"name_dict\").next_Team.fillna(\n",
    "    df_spotrac.reindex(index=df_spotrac.index[::-1]).set_index(\"name_dict\").to_dict()[\"next_Team\"]).tolist()\n",
    "\n",
    "df_offense[\"next_Season\"] = df_offense.Season + 1\n",
    "\n",
    "df_offense_2019 = df_offense.loc[df_offense.Season == 2018]\n",
    "\n",
    "df_offense_2019[list(set(df_offense_2019.columns) - set([\"Season\", \"playerID\", \"Team\", \"name\", \"next_Team\", \"Pos\"]))] = np.nan\n",
    "df_offense_2019[\"Season\"] = df_offense_2019[\"Season\"] + 1\n",
    "df_offense_2019[\"prev_Team\"] = df_offense_2019[\"Team\"]\n",
    "df_offense_2019[\"Team\"] = df_offense_2019[\"next_Team\"]\n",
    "\n",
    "df_offense = df_offense.append(df_offense_2019)\n",
    "#df_offense = df_offense.dropna(axis = 0)\n",
    "\n",
    "df_total_offense = df_offense.groupby([\"next_Season\", \"next_Team\"])[relevant_prev_columns].apply(lambda x: x.sum()).reset_index()\n",
    "\n",
    "off_columns_numeric.append('playerID')\n",
    "off_columns_numeric.append('next_Season')\n",
    "\n",
    "player_prev_merge_cols = off_columns_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns_sched2019 = [\"Week\", \"VisTm\", \"HomeTm\"]\n",
    "\n",
    "df_sched2019 = pd.read_csv(\"profootballref_schedule.csv\", usecols = relevant_columns_sched2019)\n",
    "df_sched2019 = df_sched2019.loc[~df_sched2019.Week.str.contains(\"Pre\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sched2019 = pd.merge(pd.merge(df_sched2019, df_abbreviations, left_on = \"VisTm\", right_on = \"Tm\").drop([\"VisTm\", \"Tm\"],\n",
    "                                                                                                          axis = 1),\n",
    "         df_abbreviations,\n",
    "        left_on = \"HomeTm\",\n",
    "        right_on = \"Tm\")[[\"Team_x\", \"Team_y\"]]\n",
    "\n",
    "df_sched2019 = df_sched2019.append(df_sched2019.rename({\"Team_x\": \"Team_y\", \"Team_y\": \"Team_x\"}))\n",
    "\n",
    "df_sched2019.rename({\"Team_x\": \"opp\", \"Team_y\": \"Team\"})\n",
    "\n",
    "df_sched2019.columns = [\"opp\", \"Team\"]\n",
    "\n",
    "df_sched2019[\"Season\"] = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_defense['Season'] = df_defense['Season'] + 1\n",
    "\n",
    "df_offense = pd.merge(df_offense, df_offense[player_prev_merge_cols], left_on = [\"playerID\", \"Season\"],\n",
    "                      right_on = [\"playerID\", \"next_Season\"],\n",
    "                     suffixes = [\"\", \"_player_prev\"])\n",
    "\n",
    "df_offense = pd.merge(df_offense, df_total_offense, left_on = [\"Team\", \"Season\"], right_on = [\"next_Team\", \"next_Season\"],\n",
    "                     suffixes = [\"\", \"_new_Team_prev\"])\n",
    "\n",
    "df_offense = pd.merge(df_offense, df_total_offense, left_on = [\"prev_Team\", \"Season\"], right_on = [\"next_Team\", \"next_Season\"],\n",
    "                     suffixes = [\"\", \"_old_Team_prev\"])\n",
    "\n",
    "for col in relevant_prev_columns:\n",
    "    df_offense[col + \"_Team_prev_delta\"] = df_offense[col + \"_new_Team_prev\"] - df_offense[col + \"_old_Team_prev\"]\n",
    "    \n",
    "    \n",
    "df_offense[\"exp\"] = df_offense.groupby(\"playerID\").Season.transform(lambda x: x - min(x))\n",
    "\n",
    "\n",
    "relevant_sched_columns  = off_columns_numeric_no_avg.copy()\n",
    "relevant_sched_columns.remove('points')\n",
    "\n",
    "\n",
    "df_sched = df_train.groupby([\"Season\", \"opp\"])[[\"Team\", \"game.id\"]].apply(lambda x: x.drop_duplicates(\"game.id\")).reset_index().drop([\"level_2\", \"game.id\"], axis = 1)\n",
    "\n",
    "df_prevSched =  pd.merge(df_sched,\n",
    "         (df_train.groupby([\"Season\", \"Team\"])[relevant_sched_columns].sum()/16).reset_index(),\n",
    "        left_on = [\"Season\", \"opp\"], right_on = [\"Season\", \"Team\"]).groupby([\"opp\",\"Season\"]).sum().reset_index()\n",
    " \n",
    "df_prevSched[\"Season\"] += 1    \n",
    "\n",
    "df_sched = df_sched.append(df_sched2019)\n",
    "\n",
    "df_sched.Season -= 1\n",
    "\n",
    "df_currSched =  pd.merge(df_sched,\n",
    "         (df_train.groupby([\"Season\", \"Team\"])[relevant_sched_columns].sum()/16).reset_index(),\n",
    "        left_on = [\"Season\", \"opp\"], right_on = [\"Season\", \"Team\"]).groupby([\"opp\",\"Season\"]).sum().reset_index()\n",
    " \n",
    "df_currSched[\"Season\"] += 1     \n",
    "\n",
    "df_offense = pd.merge(df_offense,\n",
    "                      df_prevSched,\n",
    "         left_on = [\"Team\", \"Season\"],\n",
    "         right_on = [\"opp\", \"Season\"],\n",
    "         suffixes = [\"\", \"_curr_sched\"])\n",
    "    \n",
    "    \n",
    "df_offense = pd.merge(df_offense, \n",
    "        df_currSched,\n",
    "         left_on = [\"prev_Team\", \"Season\"],\n",
    "         right_on = [\"opp\", \"Season\"],\n",
    "         suffixes = [\"\", \"_past_sched\"])\n",
    "\n",
    "df_defense = pd.merge(df_defense,\n",
    "                     df_currSched,\n",
    "         left_on = [\"Team\", \"Season\"],\n",
    "         right_on = [\"opp\", \"Season\"],\n",
    "         suffixes = [\"\", \"_curr_sched\"])\n",
    "\n",
    "df_defense = pd.merge(df_defense,\n",
    "                     df_prevSched,\n",
    "         left_on = [\"Team\", \"Season\"],\n",
    "         right_on = [\"opp\", \"Season\"],\n",
    "         suffixes = [\"\", \"_past_sched\"])\n",
    "\n",
    "for col in list(set(relevant_sched_columns) - set([\"Team\"])):\n",
    "    df_offense[col + \"_delta_sched\"] = df_offense[col + \"_past_sched\"] - df_offense[col + \"_curr_sched\"]\n",
    "    df_defense[col + \"_delta_sched\"] = df_defense[col + \"_past_sched\"] - df_defense[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unnecessary Values for each Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_defense_scores.columns = [\"points\"]\n",
    "\n",
    "df_defense_scores = df_defense_scores[32:len(df_defense_scores)].reset_index(drop = True)\n",
    "\n",
    "df_defense_scores[\"Team\"] = df_defense[\"Team\"]\n",
    "df_defense_scores[\"Season\"] = df_defense[\"Season\"] \n",
    "\n",
    "df_defense_scores_2018 = df_defense_scores.loc[df_defense_scores.Season == 2018]\n",
    "df_defense_scores_2018[\"Season\"] = df_defense_scores_2018[\"Season\"] + 1\n",
    "df_defense_scores_2018[\"points\"] = np.nan\n",
    "\n",
    "df_defense_scores = df_defense_scores.append(df_defense_scores_2018)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_defense = pd.merge(df_defense, df_defense_scores, on = [\"Season\", \"Team\"],suffixes = [\"_prev\", \"\"])\n",
    "\n",
    "\n",
    "\n",
    "df_offense['exp_keep'] = df_offense.exp\n",
    "df_offense['pos_keep'] = df_offense.Pos\n",
    "df_offense = pd.get_dummies(df_offense, columns = [\"exp\",\"Pos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_columns_numeric_no_avg_original = off_columns_numeric_no_avg.copy()\n",
    "\n",
    "for col in off_columns_numeric_no_avg_original:\n",
    "    off_columns_numeric_no_avg.append(col + '_per_game')\n",
    "    off_columns_numeric_no_avg.append(col + '_var')\n",
    "    off_columns_numeric_no_avg.append(col + '_career_per_game')\n",
    "    off_columns_numeric_no_avg.append(col + '_career')\n",
    "\n",
    "off_columns_numeric_no_avg.append('n_games')\n",
    "off_columns_numeric_no_avg.append('rush_per_att')\n",
    "off_columns_numeric_no_avg.append('pass_per_att')\n",
    "off_columns_numeric_no_avg.append('rec_per_att')\n",
    "off_columns_numeric_no_avg.append('pass_per_comp')\n",
    "off_columns_numeric_no_avg.append('rush_per_att')\n",
    "off_columns_numeric_no_avg.append('td_pass_per_att')\n",
    "off_columns_numeric_no_avg.append('td_pass_per_comp')\n",
    "off_columns_numeric_no_avg.append('td_rec_per_att')\n",
    "off_columns_numeric_no_avg.append('fumb_rush_per_att')\n",
    "off_columns_numeric_no_avg.append('fumb_pass_per_att')\n",
    "off_columns_numeric_no_avg.append('fumb_pass_per_att')\n",
    "off_columns_numeric_no_avg.append('td_rush_per_att')\n",
    "off_columns_numeric_no_avg.append('int_pass_per_att')\n",
    "off_columns_numeric_no_avg.append('fumb_per_rec')\n",
    "off_columns_numeric_no_avg.append('name_dict')\n",
    "off_columns_numeric_no_avg.append('next_Team')\n",
    "off_columns_numeric_no_avg.append('prev_Team')\n",
    "off_columns_numeric_no_avg.append('next_Season')\n",
    "off_columns_numeric_no_avg.append('points_per_game')\n",
    "off_columns_numeric_no_avg.append('total_games')\n",
    "\n",
    "off_columns_numeric_no_avg.remove('points')\n",
    "off_columns_numeric_no_avg.append('opp')\n",
    "\n",
    "df_offense = df_offense.drop(off_columns_numeric_no_avg, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offense_train = df_offense.loc[~df_offense.points.isna()]\n",
    "df_offense_validate = df_offense_train.loc[df_offense_train.Season == 2018]\n",
    "df_offense_train = df_offense_train.loc[df_offense_train.Season != 2018]\n",
    "df_offense_hold = df_offense.loc[df_offense.points.isna()]\n",
    "\n",
    "df_defense_train = df_defense.loc[~df_defense.points.isna()]\n",
    "df_defense_validate = df_defense_train.loc[df_defense_train.Season == 2018]\n",
    "df_defense_train = df_defense_train.loc[df_defense_train.Season != 2018]\n",
    "df_defense_hold = df_defense.loc[df_defense.points.isna()]\n",
    "\n",
    "X_train_O = df_offense_train.drop(\"points\", axis = 1)\n",
    "y_train_O = df_offense_train[\"points\"]\n",
    "\n",
    "X_train_D = df_defense_train.drop(\"points\", axis = 1)\n",
    "y_train_D = df_defense_train[\"points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureModelSelection(X_train, y_train, Pos):\n",
    "    f = open('results_' + Pos + '.txt', 'w')\n",
    "    \n",
    "    X_train = X_train.select_dtypes(exclude = object)\n",
    "\n",
    "\n",
    "    models = [LinearRegression(), Ridge(), Lasso(), XGBRegressor(objective = 'reg:squarederror', random_state = 1), \n",
    "          RandomForestRegressor(random_state = 1)]\n",
    "    \n",
    "    params = [[{\"fit_intercept\" : [True]}], \n",
    "              [{\"alpha\": [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}],\n",
    "             [{\"alpha\": [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}],\n",
    "             [{\"n_estimators\": [5, 10, 50, 100, 200], \"max_depth\": [3, 4, 5, 6, 7, 8]}],\n",
    "             [{\"n_estimators\": [5, 10, 50, 100, 200], \"max_depth\": [3, 4, 5, 6, 7, 8]}]]\n",
    "    \n",
    "    for m in range(len(models)):\n",
    "        \n",
    "        gs = GridSearchCV(models[m], params[m], n_jobs = -1, scoring = 'neg_mean_absolute_error', cv = KFold(n_splits = 5, \n",
    "                                                                                            random_state = 1)).fit( X_train, y_train)\n",
    "        \n",
    "        print(Pos)\n",
    "        print(\"\\nModel: \" + type(models[m]).__name__)\n",
    "        print(\"\\nScore: \" + str(gs.best_score_))\n",
    "        print(\"\\nParams: \" + str(gs.best_params_))\n",
    "        print('\\n')\n",
    "        print('---------------------------------------------')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        \n",
    "        print(Pos, file = f)\n",
    "        print(\"\\nModel: \" + type(models[m]).__name__, file =f )\n",
    "        print(\"\\nScore: \" + str(gs.best_score_), file = f)\n",
    "        print(\"\\nParams: \" + str(gs.best_params_), file = f)\n",
    "        print('\\n', file = f)\n",
    "        print('---------------------------------------------', file = f)\n",
    "        print('\\n', file = f)\n",
    "        \n",
    "\n",
    "    \n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O_NonPPR\n",
      "\n",
      "Model: LinearRegression\n",
      "\n",
      "Score: -38.35735253416254\n",
      "\n",
      "Params: {'fit_intercept': True}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "O_NonPPR\n",
      "\n",
      "Model: Ridge\n",
      "\n",
      "Score: -31.321731343563258\n",
      "\n",
      "Params: {'alpha': 100000}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "O_NonPPR\n",
      "\n",
      "Model: Lasso\n",
      "\n",
      "Score: -31.03322912060063\n",
      "\n",
      "Params: {'alpha': 10}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "O_NonPPR\n",
      "\n",
      "Model: XGBRegressor\n",
      "\n",
      "Score: -27.770111893860836\n",
      "\n",
      "Params: {'max_depth': 6, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "O_NonPPR\n",
      "\n",
      "Model: RandomForestRegressor\n",
      "\n",
      "Score: -28.626114262046894\n",
      "\n",
      "Params: {'max_depth': 8, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "Model: LinearRegression\n",
      "\n",
      "Score: -29.85244369506836\n",
      "\n",
      "Params: {'fit_intercept': True}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "Model: Ridge\n",
      "\n",
      "Score: -27.736384261802872\n",
      "\n",
      "Params: {'alpha': 1000}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "Model: Lasso\n",
      "\n",
      "Score: -28.138597255559894\n",
      "\n",
      "Params: {'alpha': 10}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "Model: XGBRegressor\n",
      "\n",
      "Score: -28.316487058997154\n",
      "\n",
      "Params: {'max_depth': 4, 'n_estimators': 50}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "Model: RandomForestRegressor\n",
      "\n",
      "Score: -27.740965413207277\n",
      "\n",
      "Params: {'max_depth': 8, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featureModelSelection(X_train_O, y_train_O, 'O_NonPPR') \n",
    "featureModelSelection(X_train_D, y_train_D, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_O = XGBRegressor(max_depth = 6, objective = 'reg:squarederror',\n",
    "                        n_estimators = 100, random_state = 1).fit(X_train_O.dropna().select_dtypes(exclude = object),\n",
    "                                                                      y_train_O.dropna())\n",
    "\n",
    "model_D = Lasso(alpha = 10).fit(X_train_D.dropna().select_dtypes(exclude = object),\n",
    "                                                                      y_train_D.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "\n",
    "feature_names = X_train_O.dropna().select_dtypes(exclude = object).columns\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    coefs.append([np.abs(model_O.feature_importances_[i]),\n",
    "                           feature_names[i], \n",
    "                           model_O.feature_importances_[i]])\n",
    "coefs.sort(reverse = True)\n",
    "\n",
    "coefs = pd.DataFrame(coefs)\n",
    "\n",
    "coefs = coefs.to_csv(\"output_coefs_O2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "\n",
    "feature_names = X_train_D.dropna().select_dtypes(exclude = object).columns\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    coefs.append([np.abs(model_D.coef_[i]),\n",
    "                           feature_names[i], \n",
    "                           model_D.coef_[i]])\n",
    "coefs.sort(reverse = True)\n",
    "\n",
    "coefs = pd.DataFrame(coefs)\n",
    "\n",
    "coefs = coefs.to_csv(\"output_coefs_D.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(2018, 2020):\n",
    "    df_offense_temp = df_offense.loc[df_offense.Season == season].drop('points', axis = 1).dropna()\n",
    "    \n",
    "    df_offense_temp['predicted_points'] = model_O.predict(df_offense_temp.select_dtypes(exclude = object))\n",
    "    \n",
    "    df_defense_temp = df_defense.loc[df_defense.Season == season].drop('points', axis = 1).dropna()\n",
    "    \n",
    "    df_defense_temp['predicted_points'] = model_D.predict(df_defense_temp.select_dtypes(exclude = object))\n",
    "    \n",
    "    df_offense_temp = df_offense_temp[[\"name\", \"pos_keep\", \"Team\", \"predicted_points\"]]\n",
    "    \n",
    "    df_defense_temp = df_defense_temp[[\"Team\", \"predicted_points\"]]\n",
    "    \n",
    "    df_defense_temp[\"name\"] = df_defense_temp[\"Team\"] + \"_DST\"\n",
    "    \n",
    "    df_defense_temp[\"pos_keep\"] = \"DST\"\n",
    "    \n",
    "    df_results = df_offense_temp.append(df_defense_temp)\n",
    "    \n",
    "    df_results.to_csv(\"results_\" + str(season) + '_2.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>pass.att_player_prev</th>\n",
       "      <th>pass.comp_player_prev</th>\n",
       "      <th>totalfumbs_player_prev</th>\n",
       "      <th>fumbslost_player_prev</th>\n",
       "      <th>passyds_player_prev</th>\n",
       "      <th>pass.tds_player_prev</th>\n",
       "      <th>pass.ints_player_prev</th>\n",
       "      <th>rush.att_player_prev</th>\n",
       "      <th>rushyds_player_prev</th>\n",
       "      <th>...</th>\n",
       "      <th>exp_4</th>\n",
       "      <th>exp_5</th>\n",
       "      <th>exp_6</th>\n",
       "      <th>exp_7</th>\n",
       "      <th>exp_8</th>\n",
       "      <th>exp_9</th>\n",
       "      <th>Pos_QB</th>\n",
       "      <th>Pos_RB</th>\n",
       "      <th>Pos_TE</th>\n",
       "      <th>Pos_WR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>2019</td>\n",
       "      <td>580.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5097.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Season  pass.att_player_prev  pass.comp_player_prev  \\\n",
       "3933    2019                 580.0                  383.0   \n",
       "\n",
       "      totalfumbs_player_prev  fumbslost_player_prev  passyds_player_prev  \\\n",
       "3933                     9.0                    2.0               5097.0   \n",
       "\n",
       "      pass.tds_player_prev  pass.ints_player_prev  rush.att_player_prev  \\\n",
       "3933                  50.0                   12.0                  60.0   \n",
       "\n",
       "      rushyds_player_prev   ...    exp_4  exp_5  exp_6  exp_7  exp_8  exp_9  \\\n",
       "3933                272.0   ...        0      0      0      0      0      0   \n",
       "\n",
       "      Pos_QB  Pos_RB  Pos_TE  Pos_WR  \n",
       "3933       1       0       0       0  \n",
       "\n",
       "[1 rows x 601 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_offense.loc[df_offense.Season == season].drop('points', axis = 1).dropna().loc[df_offense.name == \"P.Mahomes\"].select_dtypes(exclude = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
