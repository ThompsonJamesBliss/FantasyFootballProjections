---
title: "Fantasy Football"
author: "ThompsonBliss"
date: "August 19, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,  echo = F)
```

```{r}
library(tidyverse)
library(dplyr)
library(ggrepel)
library(knitr)

```

# Using Machine Learning to win my Fantasy Football Leagues
## By Thompson Bliss

## Section 1: Introduction

Due to growing numbers of websites and apps, as well as the greater fan interest, fantasy football participation has been increasing every year. According to Nielsen ratings, more than 15.5 million people played in a fantasy football in 2017. While many websites including [ESPN](https://fantasy.espn.com/football/players/projections), [YahooSports](https://sports.yahoo.com/2019-fantasy-football-rankings-taking-stock-at-every-position-142954685.html) and many others release projections and rankings from some of their football analysts, much of the projected points comes human perception rather than pure statistical modelling. Considering fantasy football has a scoring system based solely on player statistics, it may be interesting to see if a machine learning model can perform as well if not better than human football experts.

This project uses machine learning to predict a player's season points based on how the player has performed in previous years.

## Section 2: Proceedure
### Step 1 Acquiring the Data

Using the R package nflScrapR, player stats for each player in each game can be acquired using the 'season_player_game' command. The data is includes entries from 2009 - 2018. This data included the following statistics:

* Touchdowns
* Interceptions
* Fumbles
* Pass Yards
* Rush Yards
* Recieving Yards
* Pass Attempts
* Pass Completions
* Rush Attempts
* Receptions

Since the data did not include fantasy points, these were manually calculated using the weighted sum given by [ESPN](https://support.espn.com/hc/en-us/articles/360003914032-Scoring-Formats).

Additionally, their positions can be aquired using 'season_rosters'. The data is merged to get the position for each player. Since defences score differently from each offensive player, they are excluded from this analysis. Additionally, due to the limited data on length of kicks in nflScrapR, kickers will also be excluded. This analysis will focus on Quarterbacks, Running Backs, Wide Recievers and Tight Ends only.

### Step 2: Feature Creation

To increase accuracy of predictions, it is ideal to use more than just a player's raw stastics from the previous season. For each offensive statistic columns listed above, the following features are added:

| Feature | Description |
|-----------|-----------------------------------------------------------|
| Raw | Raw season-long value of offensive statistic from previous year |
| Per Game | Per Game value of offensive statistic from previous year |
| Var | Variance of offensive statistic per game from previous year |
| Career | Career value of offensive statistic for player up to previous year |
| Career Per Game | Career per game value of offensive statistic for player up to previous year |

In addition to these features, the following are created:

* Rush Yards per rush attempt
* Pass Yards per pass attempt
* Pass Yards per pass completion
* Recieving Yards per reception
* Rush TDs per rush attempt
* Pass TDs per pass attempt
* Pass TDs per pass completion
* Receiving TDs per reception
* Fumble per rush attempt
* Fumble per pass attempt
* Interception per pass attempt
* Fumble per reception
* Experience (years in league since 2009)
* Games played in previous season
* Total games played


Additionally, for each of these features, it is ideal to not only look at how each player individually performed, but how the team around him performed and how the team is changing. Also, players who change teams may expect to perform, differently if the team around them is significantly better or worse. Additionally, schedule has a major impact on how a player performs. If they played an easy schedule, perhaps their numbers are artificially high. Conversely, if they played a difficult schedule, their numbers could be artificially low. To take all of this into account, for each feature listed above, the following features are added:

| Feature | Description |
|-----------|-----------------------------------------------------------|
| Previous Team | Team value of feature from previous year from team player was on previous year |
| Current Team | Team value of feature from previous year from team player is on current year |
| Delta Team | Difference in Team value of feature from previous year between previous team and current team |
| Previous Schedule | Sum of average value of feature allowed by opponents played in previous schedule |
| Current Schedule | Sum of average value of feature allowed by opponents played in current schedule |
| Delta Schedule | Difference in Sum of average value of feature allowed by opponents between current schedule and previous schedule |


### Step 3: Model Fitting

Using 'GridSearchCV' in 'sklearn', a variety of models with various paramaters are explored. The data is split into three sets: Training (Seasons 2009 - 2017), Validation (2018), Test (2019). The training, validation and test data have approximately 3000, 400 and 400 rows respectively. The models are fit with the features as the independent variables and points as the dependent variable. They are tested with the least absolute deviations penalty. This penalty was chosen because RMSE penalizes larger penalties at quadratically greater rate than smaller penalties. Since there are breakout players or players who get injured who are bound to have stat lines far from what is predicted, RMSE would favor a model that more conservatively accounts for random events. The following models are trained with 2009 - 2017 data:

* Linear Regression
* Lasso Regression
* Ridge Regression
* XgBoost Regression
* Random Forest Regression

The best linear model is Lasso Regression, while the best tree-based model is XgBoost. The mean training error for lasso and XgBoost were approximately 31 and 27 per player respectively. Although these numbers seem somewhat large, if this was divided by 16 (number of games), it is an error of less than two fantasy points per game. The top 10 coefficients for lasso are shown below:

```{r fig.width=12, fig.height=10}
df_coef_O_lin <- read_csv('output_coefs_O_linear.csv') %>% head(10) %>% select(value)

df_coef_O_lin <- df_coef_O_lin %>% mutate(name = c("Player\nCareer Rush\nYards Per\nGame Up\nto Previous\nSeason",
                                  "Player\nCareer Recieving\nYards Per\nGame Up\nto Previous\nSeason",
                                  "Player\nFantasy Points\nPrevious\nSeason",
                                  "Player\nRush Yards\nPer Game\nPrevious\nSeason",
                                  "Player\nVariance of\nPass TDs\nthrown Previous\nSeason",
                                  "Player\nCareer Fantasy\nPoints up\nto Previous\nSeason",
                                  "Player\nRecieving Yards\nPrevious Season",
                                  "Team Receptions\nPrevious Season\nFor\nPrevious Team",
                                  "Player\nCareer Rush\nYards Per\nGame Up to\nPreviousSeason",
                                  "Difference in\nTeam Punt\nReturns Between\nPrevious and\nCurrent Teams")) %>%
                                  as.data.frame()



ggplot(df_coef_O_lin, aes(y = value, x = reorder(name, -abs(value)))) + geom_bar(stat = "Identity",
                                                                                 fill = "lightblue",
                                                                                 color = "blue") + theme_bw() +
  xlab('Coefficient') + ylab('Value') + ggtitle("Coefficient Values for Linear Model")  + geom_hline(yintercept = 0)

```

```{r}

df_LT <- read_csv('LT.csv')

df_LT <- df_LT %>% mutate(fantasy_points = (Yds + Yds_1)/10 + 6 * (TD + TD_1) - 2 * Fmb) %>% select(Year, fantasy_points)

```

This model appears to value career statistics slightly more than the same as previous year statistics. Two of the top ten features related to career statistics. Perhaps, the model is favoring players who have proven themselves as reliable rather than unproven players who may breakout.

Moreover, it is interesting to note that  `Career Fantasy Points` have a negative coefficient. Perhaps as good players who have accumulated a lot of points get old, they should be expected to perform worse. For example, the former Chargers running back LaDainian Tomlinson was drafted in 2001. He earned approximately 414 fantasy points in 2006, but every year after, he performed worse than the year before. Also, his final three years were his three worse totals. Thus, this is not that surprising.

Additionally, The fact that career variance is negatively correlated is not surprising as it suggests that a high yearly or career total may be the result of one or two amazing games rather than consistent play.

Finally, it is interesting that Difference in `Team Punt Returns Between Previous and Current Teams` was a top 10 feature. Perhaps this is a measure of how strong a team's defense is and how often they force punts. Teams that have many punt returns will have more offensive opportunities. Also, if they are returning the punt, that means the offense will start in better field position, which increase's a player's chance of scoring a touchdown.

Next, the top 10 most important features for XgBoost are shown below:

```{r fig.width=12, fig.height=10}
df_coef_O_tree <- read_csv('output_coefs_O_tree.csv') %>% head(10)

df_coef_O_tree <- df_coef_O_tree %>% mutate(name = c("Player\nVariance of\nFantasy Points\nPrevious Season",
                                  "Player\nFantasy Points Per\nGame Previous\nSeason",
                                  "Team Career Pass\nYards Previous\nSeason For\nPrevious Team",
                                  "Player\nFantasy Points\nPrevious Season",
                                  "Difference in\nTeam Recieving\nYards Variance\nBetween Previous\nand Current Teams",
                                  "Player\nCareer Recieving\nYards Previous\nSeason For\nPrevious Team",
                                  "Player\nCareer Pass\nTDs Per\nGame Up\nto Previous\nSeason",
                                  "Player\nCareer Rush\nYards Per\nGame Up\nto Previous\nSeason",
                                  "Team Punt Return\nTDs Previous\nSeason For\nNew Team",
                                  "Difference in\nTeam Fumbles\n Between Previous\nand Current\nSchedule")) %>%
                                  as.data.frame()

ggplot(df_coef_O_tree, aes(y = value, x = reorder(name, -abs(value)))) + geom_bar(stat = "Identity",
                                                                                 fill = "lightblue",
                                                                                 color = "blue") + theme_bw()+
  xlab('Importance') + ylab('Value') + ggtitle("Coefficient Values for Tree Model")

```

The gradient boosting model seems to use features related previous fantasy points more than raw statistics as 3 of the top 4 most important features are fantasy point related. Perhaps this importance makes sense as it is irrelevant how players earn points, but whether they are earning them.

Moreover, an important feature is `Team Punt Return TDs Previous Season For New Team`. I would imagine that similar to how `Team Punt Returns Between Previous and Current Teams` was a measure of defense and field position for the lasso model.

Finally, it is interesting to note that team `Difference in Team Fumbles Between Previous and Current Schedule` is an important feature. Anyone on the team can fumble, and even if it is not the specific player it will affect his ability to earn more points on the drive. Thus, playing a schedule with teams that force less fumbles, helps every player earn more points.

## Section 3: Model Validation

```{r}
#loading data
df_results_2018_lin <- read_csv('results_2018_linear.csv')
df_results_2019_lin <- read_csv('results_2019_linear.csv')
df_results_2018_tree <- read_csv('results_2018_tree.csv')
df_results_2019_tree <- read_csv('results_2019_tree.csv')
```

To test the accuracy of the model, the two models are tested against the actual point totals in 2018:

```{r}
df_plot <- data.frame(linear_error = abs(df_results_2018_lin$actual_points - df_results_2018_lin$predicted_points), tree_error = abs(df_results_2018_tree$actual_points - df_results_2018_tree$predicted_points)) 

df_plot_text <- data.frame(error_type = c("Linear", "Tree"), error = c(mean(df_plot$linear_error),
                                                                  mean(df_plot$tree_error)))

df_plot <- df_plot %>% gather(key = "error_type", value = "error", c("linear_error", "tree_error"))

df_plot$error_type[df_plot$error_type == "linear_error"] = "Linear Error"
df_plot$error_type[df_plot$error_type == "tree_error"] = "Tree Error"

df_plot_text <- df_plot %>% group_by(error_type) %>% summarize(error = mean(error)) %>% ungroup()

ggplot(df_plot, aes(error, color = error_type)) + geom_density(alpha=0.25) + theme_bw() +
  geom_text_repel(data = df_plot_text, aes(label = paste0("Mean: ",signif(error,4)), color = error_type), y = 0.2) +
  labs(color = "Error Type") + xlab("Error") + ylab("Density") + ggtitle("Density of Error for 2018 Season")
```

The XgBoosting model seems to do better yet again. There is a large range of errors with a maximas for each of them around their respective mean. One reason this large range occured is from players like Patrick Mahomes who based on playing one game in 2017 was incorrectly predcited by over 250 by each model. Perhaps if there was some indicator about whether a player was expected to start, it would be able to more accurately predict his success this past season. Also, players like Tarik Cohen were expected to have a major boost with better coaching, but since the model does not take quality of coaches into account, this caused error.

Moreover, it is ideal to compare the models to Yahoo and ESPN as their projections are what many use when selecting their teams. Unfortunatley, no data was availble for thier projections for each player, but it is possible to find their rankings. Below, we compare our fantasy rankings to theirs and measure the error:

```{r}

df_results_2018_yahoo <- read_csv('df_yahooSports.csv')

df_results_2018_yahoo <- df_results_2018_yahoo %>% select(Rank, `Player Name`, Team, Position) %>% rowwise() %>%
  mutate(name = paste0(substr(strsplit(`Player Name`, ' ')[[1]][1], start = 1, stop = 1), '.', strsplit(`Player Name`, ' ')[[1]][2]),
         pos_keep = Position) %>% as.data.frame() %>%
  group_by(pos_keep) %>% mutate(Rank_Yahoo = rank(Rank)) %>% ungroup() %>%
  select(Rank_Yahoo, name, pos_keep)


df_results_2018_ESPN <- read_csv('df_ESPN.csv')

df_results_2018_ESPN <- df_results_2018_ESPN %>% select(`Rank, Player`, Pos, Team) %>% rowwise() %>%
  mutate(name = paste0(substr(strsplit(`Rank, Player`, ' ')[[1]][2], start = 1, stop = 1), '.', strsplit(`Rank, Player`, ' ')[[1]][3]),
         Rank_ESPN = as.numeric(substr(strsplit(`Rank, Player`, ' ')[[1]][1], 1, nchar(strsplit(`Rank, Player`, ' ')[[1]][1]) - 1)),
         pos_keep = Pos) %>% as.data.frame()  %>%
  group_by(pos_keep) %>% mutate(Rank_ESPN = rank(Rank_ESPN)) %>% ungroup() %>% select(Rank_ESPN, name, Team, pos_keep)

df_results_2018_lin <- df_results_2018_lin %>% group_by(pos_keep) %>% mutate(Rank_lin = rank(-predicted_points),
                               Rank_actual = rank(-actual_points, ties.method = "min")) %>% ungroup() %>%
  select(Team, name, pos_keep, Team,
                                                                              Rank_actual, Rank_lin)

df_results_2018_tree <- df_results_2018_tree %>% group_by(pos_keep) %>% mutate(Rank_tree = rank(-predicted_points)) %>% ungroup %>% select(name, pos_keep, Rank_tree, Team)


df_results <- inner_join(df_results_2018_tree, df_results_2018_ESPN) %>% inner_join(df_results_2018_lin) %>%
  inner_join(df_results_2018_yahoo) %>% filter(!(name == "D.Johnson" & Team == "CLE"))

df_results_plot <- df_results %>%
  summarize(`Tree Ranking Error` = sum(abs(Rank_tree - Rank_actual))/length(Rank_actual),
            `Linear Ranking Error` = sum(abs(Rank_lin - Rank_actual))/length(Rank_actual),
            `ESPN Ranking Error` = sum(abs(Rank_ESPN - Rank_actual))/length(Rank_actual),
            `Yahoo Ranking Error` = sum(abs(Rank_Yahoo - Rank_actual))/length(Rank_actual))

df_results_plot <- df_results_plot %>% gather(key = "key", value =  "value")

ggplot(df_results_plot, aes(reorder(key, value), value)) + geom_bar(stat = "identity", color = "blue",
                                                                    fill = "lightblue") + xlab('Error Type') + ylab('Value') +
  ggtitle('Error of Rankings by Source') + theme_bw()

```

The Yahoo and ESPN model both perform better, but since they are put together by humans, they have the advantage of knowing which players will have better coaching and which players could breakout with more playtime opportunities.

## Section 4: 2019 Results

The models give the following predictions for 2019:

```{r}
names(df_results_2019_lin) <- c("Team", "NA", "Name", "Position", "Predicted Points")
df_results_2019_lin <- df_results_2019_lin[,c(1, 4, 3, 5)]
df_results_2019_lin$`Predicted Points` <- round(df_results_2019_lin$`Predicted Points`, 2)

kable(df_results_2019_lin %>% arrange(-`Predicted Points`) %>% filter(Position == "QB") %>% head(5), caption = "Top 5 Projected QBs 2019 - Linear Model")

kable(df_results_2019_lin %>% arrange(-`Predicted Points`) %>% filter(Position == "RB") %>% head(5), caption = "Top 5 Projected RBs 2019 - Linear Model")

kable(df_results_2019_lin %>% arrange(-`Predicted Points`) %>% filter(Position == "WR") %>% head(5), caption = "Top 5 Projected WRs 2019 - Linear Model")

kable(df_results_2019_lin %>% arrange(-`Predicted Points`) %>% filter(Position == "TE") %>% head(5), caption = "Top 5 Projected TEs 2019 - Linear Model")

```

```{r}
names(df_results_2019_tree) <- c("Team", "NA", "Name", "Position", "Predicted Points")
df_results_2019_tree <- df_results_2019_tree[,c(1, 4, 3, 5)]
df_results_2019_tree$`Predicted Points` <- round(df_results_2019_tree$`Predicted Points`, 2)

kable(df_results_2019_tree %>% arrange(-`Predicted Points`) %>% filter(Position == "QB") %>% head(5), caption = "Top 5 Projected QBs 2019 - Tree Model")

kable(df_results_2019_tree %>% arrange(-`Predicted Points`) %>% filter(Position == "RB") %>% head(5), caption = "Top 5 Projected RBs 2019 - Tree Model")

kable(df_results_2019_tree %>% arrange(-`Predicted Points`) %>% filter(Position == "WR") %>% head(5), caption = "Top 5 Projected WRs 2019 - Tree Model")

kable(df_results_2019_tree %>% arrange(-`Predicted Points`) %>% filter(Position == "TE") %>% head(5), caption = "Top 5 Projected TEs 2019 - Tree Model")

```

https://www.nielsen.com/us/en/insights/article/2018/fantasy-is-reality-a-look-at-growing-engagement-in-fantasy-sports/